# 算法几乎已经掌握了人类语言。为什么他们不能停止性别歧视？

> 原文:[https://dev . to/adugamayuba/algorithms-have-near-mastered-human-language-why-t-they-stop-be-sex ist-1jcn](https://dev.to/adugamayuba/algorithms-have-nearly-mastered-human-language-why-can-t-they-stop-being-sexist-1jcn)

教计算机理解人类语言曾经是一个乏味且不精确的过程。现在，语言算法分析大量文本来自学语言是如何工作的。结果可能会令人不安，比如当微软机器人 Tay 在 Twitter 上与人类接触一天后，就自学成了种族主义者。

事实证明，数据驱动的算法并不比人类更好，而且往往更糟。

“数据和数据集不是客观的；它们是人类设计的产物，”数据研究员凯特·克劳福德写道。当设计师错过或忽略模型上有偏见的数据的印记时，结果就是克劳福德所说的“信号问题”，即“数据被认为准确反映了社会世界，但存在明显的差距，来自特定社区的信号很少或没有。”

Siri、谷歌翻译和求职者跟踪系统都使用同一种算法与人类对话。像其他机器学习系统一样，NLPs(自然语言处理器或有时是“自然语言程序”的缩写)是一些代码，它们梳理人类写作的巨大宝藏，并产生其他东西——见解、建议，甚至政策建议。像所有机器学习应用一样，NLP 程序的功能与其训练数据相关联——即，告知机器对阅读材料的理解的原始信息。

偏斜数据是社会科学中一个非常古老的问题，但机器学习将它的偏见隐藏在一层混乱之下。即使是研究机器学习模型的人工智能研究人员——如神经网络，它使用加权变量来近似人脑的决策功能——也不知道偏见是如何进入他们的工作的，更不用说如何解决它了。

随着 NLP 系统渗透到数字世界的每个角落，从招聘软件到仇恨言论探测器，再到警方数据，信号问题越来越大，以适应现实世界的容器大小。每个使用机器语言解决方案的行业都有被污染的风险。算法被赋予了对医疗保健等公共服务的管辖权，这往往会加剧不平等，从而为古老的做法开脱:将最弱势群体的处境归咎于他们，以便将最好的服务重新分配给最不需要的人；试图预测哪里会发生犯罪的模型最终会让种族主义警察的行为变得更糟。

作者:阿贝尔·阿尤巴