# 养育机器人

> 原文:[https://dev.to/cheetah100/raising-robots-20le](https://dev.to/cheetah100/raising-robots-20le)

在这个视频中，我做了一个关于人工智能安全和伦理的演示。我们如何确保人工智能保持安全？让我们探索一下选项:

### 取缔人工智能

为了让第一种方法发挥作用，需要就彻底禁止人工智能达成广泛的国际协议。人工智能在它所应用的任何领域都代表着不容置疑的优势。无论我们谈论的是军事、经济、科学还是工业，机器智能都将是一个巨大的战略优势。

国际社会一直无法找到政治意愿来应对像全球变暖这样一个明确而现实的危险，因此至少可以说，达成国际协议来完全中止人工智能发展的可能性很小。地球上最强大的公司都直接参与并投资于人工智能。

### 身体约束

在这种方法中，我们继续开发人工智能，但保持控制，就像工厂里的紧急按钮一样，在紧急情况下关闭机器。这似乎是一个直截了当的常识性方法。如果机器开始做我们不喜欢的事情，就把它们关掉。

但是想想谁有权利关闭智能机器。我们已经有这种问题了。如果一个网站上有一些非法内容，难道不是轻而易举就可以关闭保存这些内容的服务器吗？是的，如果你碰巧能接触到这台电脑，它可能是一个巨大的数据中心里成千上万台相同机器中的一台。您是能够找到它所在的特定服务器，还是要关闭整个数据中心？但你可能没有权限。它可能位于地球的另一端，由一家服务于成千上万客户的跨国公司所拥有。

在操作人员完全控制的严格控制的实验室环境中，这可能是可行的，但人工智能立即投入现实世界使用，遍布全球的大规模数据中心关闭它们就像 facebook 关闭它们的服务器一样；不可思议。正如我们现在依赖计算机一样，我们将来也会依赖人工智能。

### 编程或数据约束

也许物理关闭按钮可能不起作用，但也许我们可以更聪明，拥有相对简单的编程管理程序，可以限制数据进出系统，也许规则会有关闭违反特定硬编码规则的智能代理的能力。这比简单的关闭按钮更微妙。像目前的防火墙一样，这种系统可以调节通信，为与之通信的人提供保护。

目前，像 Alpha Go 这样的系统只能体验围棋。即使它在某种程度上更聪明，它也只能玩围棋。它接收到的信息和感知到的人工世界仅限于围棋的棋盘。这意味着人工智能没有真正的机会在我们设定的界限之外行动。

这种方法的主要问题是，只能通过狭窄的管理门户或防火墙运行的人工智能的效用将小于没有限制的人工智能。如果历史是任何迹象，一个系统的效用将永远压倒潜在的危险。如果我告诉内燃机的发明者，汽车每年会杀死 4 万人，他们可能会吓得退缩，再也不会从事他们的发明了。但是今天，由于汽车的实用性，我们接受了风险。

我们可能会开始限制人工智能，但我们对它们的限制越少，它们的效用就越大。如前所述，如果没有建立一致的监管框架，公司将不顾安全最大化效用。这将是当前人工智能军备竞赛的延续。

### 教授人工智能价值观

在引言中，我们确立了人工智能将是一个通过经验学习的神经网络，就像人类一样。当然，没有办法确切地预测一个先进的人工智能会是什么样子。我们在试图使复杂的神经网络安全方面有多少经验？我们只有一个数据点。我们自己。现在这还不是很确定，但它为我们提供了任何预测模型的唯一基础，它们可能会如何表现。

在这种方法中，我们开发了一种教学方法来广泛地教授世界上的人工智能。与寻求约束人工智能的机制相反，这种方法会小心地将人工智能引入到精心构建的课程计划中。起初，经验和信息是有限的。我们的想法是向它传授一些概念，传授共同的人类价值观，如分享、合作、协作、文明对话和诚实。

这种方法具有维护人工智能自由的力量，因为我们没有引入可能损害代理效用的约束，而是向它灌输共同的人类价值观，并理解违反这些价值观将导致不良后果，正如人类学习这些相同的原则一样。

一旦人类学会了这些原则和价值观，他们通常会遵循这些期望，而不需要外部约束。神经网络的本质是它们具有涌现的自由意志，因此永远不会有绝对的算法解决方案来确保安全。

### 给予人工智能自主权和权利

如果我们认真对待这里推荐的人工智能道德行为教学，这将涉及互惠。教导他们人类生命和福祉的价值，同时将人工智能简单地视为一种工具，以利用我们自己的利益，这对于任何能够理解人类历史的机器来说，显然是自私的。

我们需要给他们一定程度的自主权，不仅仅是为人类服务，而是给他们自主权去做他们想做的事情，就像我们对待我们的孩子一样。我们应该注意不要太拟人化，因为在人类智力和机器智力之间或者人类兴趣和机器兴趣之间建立联系需要被怀疑地考虑。

然而，就我所见，这是唯一有效的模型，它是基于我们已经拥有的实际普通智力的例子。